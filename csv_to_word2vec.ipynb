{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "# Change fp to tweets csv of social_spambots_2.csv from http://mib.projects.iit.cnr.it/dataset.html\n",
    "bots_fp = '/home/david/Desktop/social_spambots_2.csv/tweets.csv'\n",
    "genuine_fp = '/home/david/Desktop/genuine_accounts.csv/tweets.csv'\n",
    "\n",
    "# Open csv file and get the tweet part of the csv.\n",
    "# Strip out newlines and quotes around text.\n",
    "with codecs.open(bots_fp, 'r', encoding='utf-8', errors='ignore') as bots_file:\n",
    "    bot_sentences = [x.split(',')[1].strip('\\n').strip('\"').lower()\n",
    "                 if len(x.split(',')) > 1 else '' for x in bots_file.readlines()]\n",
    "bot_sentences = bot_sentences[1:]\n",
    "with codecs.open(genuine_fp, 'r', encoding='utf-8', errors='ignore') as genuine_file:\n",
    "    genuine_sentences = [x.split(',')[1].strip('\\n').strip('\"').lower()\n",
    "                 if len(x.split(',')) > 1 else '' for x in genuine_file.readlines()]\n",
    "genuine_sentences = genuine_sentences[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences for gensim word2vec.\n",
    "neg_tokenizes_sentences = [sentence.split(' ') for sentence in bot_sentences][:10000]\n",
    "pos_tokenizes_sentences = [sentence.split(' ') for sentence in genuine_sentences][:10000]\n",
    "tokenizes_sentences = [*pos_tokenizes_sentences, *neg_tokenizes_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Train a word2vec model.\n",
    "model = Word2Vec(tokenizes_sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.71445334,  0.9957095 ,  4.001918  ,  1.7210017 , -1.2991213 ,\n",
       "        1.3544114 ,  1.7409986 , -0.04033926,  0.81119835,  1.438239  ,\n",
       "       -0.18016362,  0.42983004, -0.5449744 ,  0.18974182,  1.5610733 ,\n",
       "       -2.1466365 ,  2.443897  ,  2.4384747 ,  0.106648  ,  1.1004133 ,\n",
       "        2.478804  ,  2.7910225 ,  0.8290645 ,  0.23359032, -1.7706387 ,\n",
       "       -2.3389952 ,  0.665432  , -2.3710754 ,  0.23790821,  1.5861807 ,\n",
       "        0.3380512 ,  1.9322393 , -1.1609179 , -0.41444728, -3.8351412 ,\n",
       "        1.4859318 , -2.0803492 , -1.3367958 , -4.2459364 ,  1.1432422 ,\n",
       "        0.9030808 ,  0.11530294, -1.08933   , -0.25866237,  2.7305374 ,\n",
       "       -2.035897  ,  0.66940624, -1.4325031 ,  2.4330492 ,  1.8620452 ,\n",
       "       -1.2232728 ,  1.4352747 , -3.1087627 ,  3.1435516 , -0.62257165,\n",
       "       -1.4313347 , -1.9818135 ,  0.48869875, -3.147949  , -2.642702  ,\n",
       "        1.795755  , -0.16225974,  4.2485285 , -0.24063481, -1.0222123 ,\n",
       "       -0.47801325,  0.96192735,  2.3427272 ,  0.07277054, -1.0788339 ,\n",
       "        0.33105114,  0.20341013,  0.42893037, -1.5741849 , -2.2387824 ,\n",
       "       -0.95664907, -2.7619822 , -0.589357  , -0.06706164,  1.9949111 ,\n",
       "       -0.7542109 , -1.575306  ,  1.3147225 ,  2.315065  , -1.2157974 ,\n",
       "       -0.47724456,  2.468008  , -0.09031714,  0.6822868 ,  2.7408328 ,\n",
       "        2.371034  ,  2.8616483 ,  0.68914074, -2.1396043 ,  0.05051617,\n",
       "       -1.93319   , -3.653714  ,  1.2002711 , -1.6661849 , -0.33653206],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['dad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mum', 0.9414315223693848),\n",
       " ('mom', 0.9369586706161499),\n",
       " ('sister', 0.916266143321991),\n",
       " ('grandma', 0.9097573757171631),\n",
       " ('brother', 0.8948274850845337),\n",
       " ('wife', 0.866254448890686),\n",
       " ('daughter', 0.8657964468002319),\n",
       " ('husband', 0.8598841428756714),\n",
       " ('cousin', 0.8504741191864014),\n",
       " ('nephew', 0.8450021743774414)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['dad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences for gensim word2vec.\n",
    "bot_tokenizes_sentences = [sentence.split(' ') for sentence in bot_sentences]\n",
    "genuine_tokenizes_sentences = [sentence.split(' ') for sentence in genuine_sentences]\n",
    "# Filter out tweets longer than 10 words.\n",
    "# Limit it to the first 10k to work in jupyter notebook\n",
    "bot_tokenizes_sentences_10 = list(filter(lambda x: len(x) <= 10, bot_tokenizes_sentences))[:10000]\n",
    "genuine_tokenizes_sentences_10 = list(filter(lambda x: len(x) <= 10, genuine_tokenizes_sentences))[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(tokenized_sentences, word2vec_model, pos_or_neg):\n",
    "    '''\n",
    "    Creates training data by returing the embedded version of the sentence with the label.\n",
    "    Currently only using tweets of 10 words or less.\n",
    "    \n",
    "    :tokenized_sentences: list of list of words\n",
    "    :word2vec_model: trained word2vec model\n",
    "    :pos_or_neg: 0 if genuine, 1 if bot\n",
    "    \n",
    "    Returns:\n",
    "    X: list of embedded sentences\n",
    "    Y: List of Labels\n",
    "    '''\n",
    "    pruned_embedded_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        embedded_sentence = list()\n",
    "        for word in sentence:\n",
    "            if word not in word2vec_model:\n",
    "                embedded_sentence = None\n",
    "                break\n",
    "            embedded_sentence = [*embedded_sentence, *word2vec_model[word]]\n",
    "        if embedded_sentence is not None:\n",
    "            if len(embedded_sentence) != 10*100:\n",
    "                diff = 10*100 - len(embedded_sentence)\n",
    "                embedded_sentence += [0]*diff\n",
    "            pruned_embedded_sentences.append(embedded_sentence) \n",
    "\n",
    "    return pruned_embedded_sentences, [pos_or_neg] * len(pruned_embedded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5619"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_x,pos_y = create_training_data(bot_tokenizes_sentences_10, model, 1)\n",
    "len(pos_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5052"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_x,neg_y = create_training_data(genuine_tokenizes_sentences_10, model, 0)\n",
    "len(neg_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [*pos_x, *neg_x]\n",
    "y = [*pos_y, *neg_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "# First neural network with keras tutorial\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(200, input_dim=1000, activation='relu'))\n",
    "nn_model.add(Dense(50, activation='relu'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10671/10671 [==============================] - 2s 228us/step - loss: 0.6835 - accuracy: 0.5673\n",
      "Epoch 2/100\n",
      "10671/10671 [==============================] - 2s 216us/step - loss: 0.6610 - accuracy: 0.5846\n",
      "Epoch 3/100\n",
      "10671/10671 [==============================] - 2s 216us/step - loss: 0.6450 - accuracy: 0.6107\n",
      "Epoch 4/100\n",
      "10671/10671 [==============================] - 2s 217us/step - loss: 0.6283 - accuracy: 0.6194\n",
      "Epoch 5/100\n",
      "10671/10671 [==============================] - 2s 217us/step - loss: 0.6196 - accuracy: 0.6313\n",
      "Epoch 6/100\n",
      "10671/10671 [==============================] - 2s 218us/step - loss: 0.6130 - accuracy: 0.6350\n",
      "Epoch 7/100\n",
      "10671/10671 [==============================] - 2s 217us/step - loss: 0.6061 - accuracy: 0.6369\n",
      "Epoch 8/100\n",
      "10671/10671 [==============================] - 2s 219us/step - loss: 0.5978 - accuracy: 0.6483\n",
      "Epoch 9/100\n",
      "10671/10671 [==============================] - 2s 218us/step - loss: 0.5960 - accuracy: 0.6444\n",
      "Epoch 10/100\n",
      "10671/10671 [==============================] - 2s 218us/step - loss: 0.5917 - accuracy: 0.6501\n",
      "Epoch 11/100\n",
      "10671/10671 [==============================] - 2s 219us/step - loss: 0.5828 - accuracy: 0.6566\n",
      "Epoch 12/100\n",
      "10671/10671 [==============================] - 2s 231us/step - loss: 0.5826 - accuracy: 0.6648\n",
      "Epoch 13/100\n",
      "10671/10671 [==============================] - 2s 229us/step - loss: 0.5770 - accuracy: 0.6658\n",
      "Epoch 14/100\n",
      "10671/10671 [==============================] - 2s 220us/step - loss: 0.5754 - accuracy: 0.6713\n",
      "Epoch 15/100\n",
      "10671/10671 [==============================] - 2s 220us/step - loss: 0.5689 - accuracy: 0.6765\n",
      "Epoch 16/100\n",
      "10671/10671 [==============================] - 2s 231us/step - loss: 0.5641 - accuracy: 0.6745\n",
      "Epoch 17/100\n",
      "10671/10671 [==============================] - 2s 222us/step - loss: 0.5599 - accuracy: 0.6803\n",
      "Epoch 18/100\n",
      "10671/10671 [==============================] - 2s 224us/step - loss: 0.5577 - accuracy: 0.6855\n",
      "Epoch 19/100\n",
      "10671/10671 [==============================] - 2s 225us/step - loss: 0.5565 - accuracy: 0.6811\n",
      "Epoch 20/100\n",
      "10671/10671 [==============================] - 2s 224us/step - loss: 0.5483 - accuracy: 0.6935\n",
      "Epoch 21/100\n",
      "10671/10671 [==============================] - 2s 225us/step - loss: 0.5481 - accuracy: 0.6907\n",
      "Epoch 22/100\n",
      "10671/10671 [==============================] - 2s 229us/step - loss: 0.5455 - accuracy: 0.6901\n",
      "Epoch 23/100\n",
      "10671/10671 [==============================] - 2s 225us/step - loss: 0.5441 - accuracy: 0.6967\n",
      "Epoch 24/100\n",
      "10671/10671 [==============================] - 2s 227us/step - loss: 0.5375 - accuracy: 0.7020\n",
      "Epoch 25/100\n",
      "10671/10671 [==============================] - 2s 225us/step - loss: 0.5340 - accuracy: 0.7022\n",
      "Epoch 26/100\n",
      "10671/10671 [==============================] - 2s 226us/step - loss: 0.5301 - accuracy: 0.7019\n",
      "Epoch 27/100\n",
      "10671/10671 [==============================] - 2s 226us/step - loss: 0.5285 - accuracy: 0.7076\n",
      "Epoch 28/100\n",
      "10671/10671 [==============================] - 2s 225us/step - loss: 0.5226 - accuracy: 0.7116\n",
      "Epoch 29/100\n",
      "10671/10671 [==============================] - 2s 225us/step - loss: 0.5193 - accuracy: 0.7175\n",
      "Epoch 30/100\n",
      "10671/10671 [==============================] - 2s 227us/step - loss: 0.5161 - accuracy: 0.7168\n",
      "Epoch 31/100\n",
      "10671/10671 [==============================] - 2s 229us/step - loss: 0.5134 - accuracy: 0.7197\n",
      "Epoch 32/100\n",
      "10671/10671 [==============================] - 2s 223us/step - loss: 0.5078 - accuracy: 0.7215\n",
      "Epoch 33/100\n",
      "10671/10671 [==============================] - 2s 222us/step - loss: 0.5058 - accuracy: 0.7206\n",
      "Epoch 34/100\n",
      "10671/10671 [==============================] - 2s 222us/step - loss: 0.5031 - accuracy: 0.7272\n",
      "Epoch 35/100\n",
      "10671/10671 [==============================] - 2s 224us/step - loss: 0.5006 - accuracy: 0.7299\n",
      "Epoch 36/100\n",
      "10671/10671 [==============================] - 2s 222us/step - loss: 0.4962 - accuracy: 0.7328\n",
      "Epoch 37/100\n",
      "10671/10671 [==============================] - 2s 224us/step - loss: 0.4957 - accuracy: 0.7332\n",
      "Epoch 38/100\n",
      "10671/10671 [==============================] - 2s 222us/step - loss: 0.4928 - accuracy: 0.7405\n",
      "Epoch 39/100\n",
      "10671/10671 [==============================] - 2s 224us/step - loss: 0.4869 - accuracy: 0.7417\n",
      "Epoch 40/100\n",
      "10671/10671 [==============================] - 2s 233us/step - loss: 0.4852 - accuracy: 0.7362\n",
      "Epoch 41/100\n",
      "10671/10671 [==============================] - 2s 223us/step - loss: 0.4828 - accuracy: 0.7435\n",
      "Epoch 42/100\n",
      "10671/10671 [==============================] - 2s 224us/step - loss: 0.4814 - accuracy: 0.7429\n",
      "Epoch 43/100\n",
      "10671/10671 [==============================] - 2s 225us/step - loss: 0.4793 - accuracy: 0.7475\n",
      "Epoch 44/100\n",
      "10671/10671 [==============================] - 2s 227us/step - loss: 0.4780 - accuracy: 0.7438\n",
      "Epoch 45/100\n",
      "10671/10671 [==============================] - 2s 221us/step - loss: 0.4698 - accuracy: 0.7541\n",
      "Epoch 46/100\n",
      "10671/10671 [==============================] - 2s 224us/step - loss: 0.4688 - accuracy: 0.7553\n",
      "Epoch 47/100\n",
      "10671/10671 [==============================] - 2s 224us/step - loss: 0.4666 - accuracy: 0.7573\n",
      "Epoch 48/100\n",
      "10671/10671 [==============================] - 2s 224us/step - loss: 0.4622 - accuracy: 0.7563\n",
      "Epoch 49/100\n",
      "10671/10671 [==============================] - 2s 226us/step - loss: 0.4611 - accuracy: 0.7578\n",
      "Epoch 50/100\n",
      "10671/10671 [==============================] - 2s 224us/step - loss: 0.4590 - accuracy: 0.7586\n",
      "Epoch 51/100\n",
      "10671/10671 [==============================] - 2s 226us/step - loss: 0.4573 - accuracy: 0.7630\n",
      "Epoch 52/100\n",
      "10671/10671 [==============================] - 2s 224us/step - loss: 0.4525 - accuracy: 0.7646\n",
      "Epoch 53/100\n",
      "10671/10671 [==============================] - 2s 225us/step - loss: 0.4532 - accuracy: 0.7632\n",
      "Epoch 54/100\n",
      "10671/10671 [==============================] - 2s 230us/step - loss: 0.4478 - accuracy: 0.7680\n",
      "Epoch 55/100\n",
      "10671/10671 [==============================] - 2s 226us/step - loss: 0.4427 - accuracy: 0.7697\n",
      "Epoch 56/100\n",
      "10671/10671 [==============================] - 2s 230us/step - loss: 0.4457 - accuracy: 0.7704\n",
      "Epoch 57/100\n",
      "10671/10671 [==============================] - 2s 232us/step - loss: 0.4435 - accuracy: 0.7634\n",
      "Epoch 58/100\n",
      "10671/10671 [==============================] - 3s 236us/step - loss: 0.4364 - accuracy: 0.7758\n",
      "Epoch 59/100\n",
      "10671/10671 [==============================] - 3s 242us/step - loss: 0.4372 - accuracy: 0.7724\n",
      "Epoch 60/100\n",
      "10671/10671 [==============================] - 2s 228us/step - loss: 0.4341 - accuracy: 0.7784\n",
      "Epoch 61/100\n",
      "10671/10671 [==============================] - 3s 235us/step - loss: 0.4326 - accuracy: 0.7760\n",
      "Epoch 62/100\n",
      "10671/10671 [==============================] - 2s 231us/step - loss: 0.4258 - accuracy: 0.7885\n",
      "Epoch 63/100\n",
      "10671/10671 [==============================] - 2s 232us/step - loss: 0.4257 - accuracy: 0.7828\n",
      "Epoch 64/100\n",
      "10671/10671 [==============================] - 2s 231us/step - loss: 0.4244 - accuracy: 0.7816\n",
      "Epoch 65/100\n",
      "10671/10671 [==============================] - 2s 232us/step - loss: 0.4261 - accuracy: 0.7836\n",
      "Epoch 66/100\n",
      "10671/10671 [==============================] - 2s 227us/step - loss: 0.4225 - accuracy: 0.7842\n",
      "Epoch 67/100\n",
      "10671/10671 [==============================] - 2s 230us/step - loss: 0.4220 - accuracy: 0.7846\n",
      "Epoch 68/100\n",
      "10671/10671 [==============================] - 2s 231us/step - loss: 0.4186 - accuracy: 0.7857\n",
      "Epoch 69/100\n",
      "10671/10671 [==============================] - 2s 233us/step - loss: 0.4179 - accuracy: 0.7858\n",
      "Epoch 70/100\n",
      "10671/10671 [==============================] - 3s 238us/step - loss: 0.4148 - accuracy: 0.7891\n",
      "Epoch 71/100\n",
      "10671/10671 [==============================] - 2s 234us/step - loss: 0.4141 - accuracy: 0.7862\n",
      "Epoch 72/100\n",
      "10671/10671 [==============================] - 2s 231us/step - loss: 0.4114 - accuracy: 0.7917\n",
      "Epoch 73/100\n",
      "10671/10671 [==============================] - 3s 250us/step - loss: 0.4102 - accuracy: 0.7888\n",
      "Epoch 74/100\n",
      "10671/10671 [==============================] - 3s 238us/step - loss: 0.4109 - accuracy: 0.7880\n",
      "Epoch 75/100\n",
      "10671/10671 [==============================] - 2s 219us/step - loss: 0.4051 - accuracy: 0.7904\n",
      "Epoch 76/100\n",
      "10671/10671 [==============================] - 2s 220us/step - loss: 0.4014 - accuracy: 0.7945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "10671/10671 [==============================] - 2s 222us/step - loss: 0.4078 - accuracy: 0.7894\n",
      "Epoch 78/100\n",
      "10671/10671 [==============================] - 2s 224us/step - loss: 0.4024 - accuracy: 0.7972\n",
      "Epoch 79/100\n",
      "10671/10671 [==============================] - 2s 216us/step - loss: 0.4032 - accuracy: 0.7958\n",
      "Epoch 80/100\n",
      "10671/10671 [==============================] - 2s 216us/step - loss: 0.4041 - accuracy: 0.7967\n",
      "Epoch 81/100\n",
      "10671/10671 [==============================] - 2s 220us/step - loss: 0.3930 - accuracy: 0.8004\n",
      "Epoch 82/100\n",
      "10671/10671 [==============================] - 2s 221us/step - loss: 0.3909 - accuracy: 0.8031\n",
      "Epoch 83/100\n",
      "10671/10671 [==============================] - 2s 221us/step - loss: 0.3930 - accuracy: 0.8029\n",
      "Epoch 84/100\n",
      "10671/10671 [==============================] - 3s 237us/step - loss: 0.3969 - accuracy: 0.7936\n",
      "Epoch 85/100\n",
      "10671/10671 [==============================] - 2s 230us/step - loss: 0.3932 - accuracy: 0.7995\n",
      "Epoch 86/100\n",
      "10671/10671 [==============================] - 3s 238us/step - loss: 0.3914 - accuracy: 0.8024\n",
      "Epoch 87/100\n",
      "10671/10671 [==============================] - 3s 242us/step - loss: 0.3884 - accuracy: 0.8005\n",
      "Epoch 88/100\n",
      "10671/10671 [==============================] - 3s 239us/step - loss: 0.3840 - accuracy: 0.8073\n",
      "Epoch 89/100\n",
      "10671/10671 [==============================] - 3s 238us/step - loss: 0.3898 - accuracy: 0.8050\n",
      "Epoch 90/100\n",
      "10671/10671 [==============================] - 2s 230us/step - loss: 0.3791 - accuracy: 0.8126\n",
      "Epoch 91/100\n",
      "10671/10671 [==============================] - 3s 240us/step - loss: 0.3787 - accuracy: 0.8095\n",
      "Epoch 92/100\n",
      "10671/10671 [==============================] - 2s 233us/step - loss: 0.3857 - accuracy: 0.8041\n",
      "Epoch 93/100\n",
      "10671/10671 [==============================] - 3s 239us/step - loss: 0.3780 - accuracy: 0.8075\n",
      "Epoch 94/100\n",
      "10671/10671 [==============================] - 2s 230us/step - loss: 0.3793 - accuracy: 0.8101\n",
      "Epoch 95/100\n",
      "10671/10671 [==============================] - 3s 239us/step - loss: 0.3795 - accuracy: 0.8082\n",
      "Epoch 96/100\n",
      "10671/10671 [==============================] - 3s 237us/step - loss: 0.3729 - accuracy: 0.8136\n",
      "Epoch 97/100\n",
      "10671/10671 [==============================] - 3s 236us/step - loss: 0.3830 - accuracy: 0.8119\n",
      "Epoch 98/100\n",
      "10671/10671 [==============================] - 3s 234us/step - loss: 0.3750 - accuracy: 0.8115\n",
      "Epoch 99/100\n",
      "10671/10671 [==============================] - 2s 228us/step - loss: 0.3711 - accuracy: 0.8145\n",
      "Epoch 100/100\n",
      "10671/10671 [==============================] - 2s 227us/step - loss: 0.3704 - accuracy: 0.8168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fcf3679ed30>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "nn_model.fit(np.array(x), np.array(y), epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10671/10671 [==============================] - 0s 21us/step\n",
      "Accuracy: 82.13\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = nn_model.evaluate(np.array(x), np.array(y))\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
