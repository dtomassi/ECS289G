{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "# Change fp to tweets csv of social_spambots_2.csv from http://mib.projects.iit.cnr.it/dataset.html\n",
    "bots_fp = '/home/david/Desktop/social_spambots_2.csv/tweets.csv'\n",
    "genuine_fp = '/home/david/Desktop/genuine_accounts.csv/tweets.csv'\n",
    "\n",
    "# Open csv file and get the tweet part of the csv.\n",
    "# Strip out newlines and quotes around text.\n",
    "with codecs.open(bots_fp, 'r', encoding='utf-8', errors='ignore') as bots_file:\n",
    "    bot_sentences = [x.split(',')[1].strip('\\n').strip('\"').lower()\n",
    "                 if len(x.split(',')) > 1 else '' for x in bots_file.readlines()]\n",
    "bot_sentences = bot_sentences[1:]\n",
    "with codecs.open(genuine_fp, 'r', encoding='utf-8', errors='ignore') as genuine_file:\n",
    "    genuine_sentences = [x.split(',')[1].strip('\\n').strip('\"').lower()\n",
    "                 if len(x.split(',')) > 1 else '' for x in genuine_file.readlines()]\n",
    "genuine_sentences = genuine_sentences[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences for gensim word2vec.\n",
    "tokenizes_sentences = [sentence.split(' ') for sentence in genuine_sentences]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2860295\n",
      "['rt', '@morningjewshow:', 'speaking', 'about', 'jews', 'and', 'comedy', 'tonight', 'at', 'temple', 'emanu-el', 'in', 'san', 'francisco.', 'in', 'other', 'words']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizes_sentences))\n",
    "print(tokenizes_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Train a word2vec model.\n",
    "model = Word2Vec(tokenizes_sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " '@morningjewshow:',\n",
       " 'speaking',\n",
       " 'about',\n",
       " 'jews',\n",
       " 'and',\n",
       " 'comedy',\n",
       " 'tonight',\n",
       " 'at',\n",
       " 'temple']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.wv.vocab)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.5086179e+00, -6.4968365e-01, -2.3148289e+00,  1.0698880e-01,\n",
       "       -7.3977137e-01, -1.4446214e-01,  2.9432261e+00,  9.9253863e-01,\n",
       "       -3.4304583e-01,  5.0636637e-01, -3.0197265e+00,  8.2303566e-01,\n",
       "        2.9956572e+00, -5.1384866e-01, -8.4286642e-01, -3.8677725e-05,\n",
       "        2.2685297e+00,  2.5673730e+00, -5.1027727e-01,  8.0755931e-01,\n",
       "        7.7935171e-01,  3.2813114e-01,  1.3714983e+00,  4.6107167e-01,\n",
       "       -7.2718924e-01, -8.3830500e-01,  5.2966356e-01,  1.6572822e+00,\n",
       "       -4.6613817e+00,  1.9734647e+00,  1.5797269e+00,  1.2802039e+00,\n",
       "       -1.4272243e+00,  1.1769537e-01,  2.7637258e+00, -1.0508411e+00,\n",
       "        4.9056172e-01, -1.7329830e+00,  2.5264430e+00,  1.6759747e+00,\n",
       "        6.6929770e-01, -8.0651945e-01,  1.4680361e+00, -3.0158200e+00,\n",
       "        9.4583440e-01, -1.6171439e+00, -1.9272529e+00,  1.2858894e+00,\n",
       "       -2.4988508e+00,  2.2832789e+00,  2.5144823e+00,  2.0674613e+00,\n",
       "       -3.0779338e+00,  9.1380161e-01,  2.0189592e-01,  6.4543658e-01,\n",
       "       -3.0706620e-01,  1.2531377e+00,  7.1967864e-01, -3.1507821e+00,\n",
       "        1.6019680e-01, -1.4700041e+00,  2.6674873e-01, -1.4871938e+00,\n",
       "       -7.0558691e-01, -1.6843208e+00, -1.9590560e+00,  9.1560215e-01,\n",
       "        2.5018506e+00,  3.0034959e+00, -1.5497078e+00, -2.3187084e+00,\n",
       "       -1.4645651e-01,  1.9714489e+00, -1.0702009e+00,  1.6244190e+00,\n",
       "       -3.0662999e-01,  2.5657538e-01, -3.9673626e-01, -3.1898234e+00,\n",
       "        9.7563893e-02,  4.3456850e+00,  2.5895641e+00,  8.5991073e-01,\n",
       "       -1.0513473e+00,  8.0419034e-01, -1.8349218e+00, -9.5987739e-03,\n",
       "        6.4748579e-01, -9.3215382e-01,  1.0269344e+00,  2.4271317e+00,\n",
       "        1.8065737e+00,  2.4870803e+00,  8.2503736e-01, -2.3378639e+00,\n",
       "        1.4424061e+00,  2.2754796e+00,  1.0560383e+00, -1.1258506e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['dad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mum', 0.9454367160797119),\n",
       " ('mom', 0.9385337829589844),\n",
       " ('sister', 0.9177341461181641),\n",
       " ('grandma', 0.9141216278076172),\n",
       " ('brother', 0.881198525428772),\n",
       " ('daughter', 0.8643038272857666),\n",
       " ('husband', 0.8602260947227478),\n",
       " ('wife', 0.8548371195793152),\n",
       " ('cousin', 0.8535829782485962),\n",
       " ('nephew', 0.8311100602149963)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['dad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(x) for x in tokenizes_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 9.737832985758462\n",
      "median 8\n",
      "max 131\n",
      "mean 9.880279685691889\n",
      "median 9.0\n",
      "max 78\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "print('mean', statistics.mean(lens))\n",
    "print('median', statistics.median(lens))\n",
    "print('max', max(lens))\n",
    "# Tokenize sentences for gensim word2vec.\n",
    "bot_tokenizes_sentences = [sentence.split(' ') for sentence in bot_sentences]\n",
    "bot_lens = [len(x) for x in bot_tokenizes_sentences]\n",
    "print('mean', statistics.mean(bot_lens))\n",
    "print('median', statistics.median(bot_lens))\n",
    "print('max', max(bot_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences for gensim word2vec.\n",
    "genuine_tokenizes_sentences = [sentence.split(' ') for sentence in genuine_sentences]\n",
    "bot_tokenizes_sentences_10 = list(filter(lambda x: len(x) <= 10, bot_tokenizes_sentences))\n",
    "genuine_tokenizes_sentences_10 = list(filter(lambda x: len(x) <= 10, genuine_tokenizes_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1771034"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genuine_tokenizes_sentences_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "def create_training_data(tokenized_sentences, word2vec_model, pos_or_neg):\n",
    "    '''\n",
    "    Creates training data by returing the embedded version of the sentence with the label.\n",
    "    Currently only using tweets of 10 words or less.\n",
    "    \n",
    "    :tokenized_sentences: list of list of words\n",
    "    :word2vec_model: trained word2vec model\n",
    "    :pos_or_neg: 0 if genuine, 1 if bot\n",
    "    \n",
    "    Returns:\n",
    "    X: list of embedded sentences\n",
    "    Y: List of Labels\n",
    "    '''\n",
    "    # TODO: Prune tokenized_sentences that do not have words in the word2vec_model\n",
    "    pruned_tokenized_sentences = []\n",
    "    # TODO: Create embedding via word2vec model and pad 0 vectors for sentences less than 10\n",
    "\n",
    "    X = []\n",
    "    Y = [pos_or_neg] * len(pruned_tokenized_sentences)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
