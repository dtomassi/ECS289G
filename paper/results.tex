\section{Results}
\label{sec:results}
\iffalse
Three metrics, in addition to classification accuracy, that are commonly required for a neural network model on a binary classification problem are:
\begin{itemize}
\item Precision: This is the ratio of the true positives to true positives and false positives (true positives / (true positive + false positive)). This is percentage of our results which are relevant.
\item Recall: This is the ratio of the true positives to true positives and false positives (true positives / (true positive + false negative)). This refers to the percentage of total relevant results correctly classified by our algorithm.
\item F1 score: F1 is a metric that takes into account both precision and recall. This number can be maximized to make our model better. F1 score = $2$ *(precision * recall)/(precision + recall)
ds

\end{itemize}
\fi
\subsection{Feed-Forward Neural Network}
Precision and recall are extremely important metrics to evaluate out model. Unfortunately, it is not possible to maximize both these scores at once. For problems for which both precision and recall are important, we can maximize the F1 score.

\subsubsection{Single Hidden Layer} Since we did obtain a high ``Accuracy'' with manually pre-processing the data, we used the tool Ekphrais for the same. Without Ekphrasis we got an accuracy of $36.08$\%. With Ekphrais, we got a accuracy of $87.3$\% for a single hidden layer feed-forward network.

\subsubsection{Multiple Hidden Layers} Without Ekphrasis we got an accuracy of $45$\%. With Ekphrasis we obtained and acuracy of $93.5$\% for multiple hidden layer
feed-forward neural network.

\subsection{Long Short-Term Memory}
This model achieved accuracy of $83.5$\% on the testing set and $89$\% on the validation set. Results of the modification in hyper-parameters and also the model architecture with pre-trained emending that all came with worse results are mention in the discussion section. 
