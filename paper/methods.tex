\section{Methods}
\label{sec:methods}

\subsection{Dataset} The dataset we will be using for our training and
evaluation will be the My Information Bubble (MIB) dataset
\citep{Cresci:2017:PSS:3041021.3055135}. It is a collection of genuine
and spam tweets from bot accounts. There are thousands of accounts
with millions of tweets. We will split the dataset by accounts with a
90/10 split for training and validation data.

\subsection{Pre-Processing} We will pre-process the tweets in order to
normalize their format and reduce vocabulary size when we create an
embedding.

\subsubsection{Parsing} To parse the tweets, we employ the
text-processing tool Ekphrasis
\citep{baziotis-pelekis-doulkeridis:2017:SemEval2} that will performn
tokenization, normalization and word segmentation. Using Ekphrasis
allows for a uniform format for the tweets and abstracts away
usernames and emojis which will decress the vocabulary size that will
be used in the embedding.

\subsubsection{Embedding} Since our vocabulary size will be large as
the breadth of language, slang, and domain specific words is diverse.
Using a one-hot-encoding, would increase the dimensonality of the
input by a large margin. To overcome this obstricle, we will be use a
Word2Vec embedding \citep{Mikolov:2013:DRW:2999792.2999959} which will
decrease the dimensonality to 100 to represent one word. In particular
we will train a Word2Vec
model\footnote{https://radimrehurek.com/gensim/models/word2vec.html}
on our tweets and use the embedding to encode them for input to the
model.

\subsection{Pruning} We will prune tweets from our dataset that fall
outside the vocabular that was learning from our Word2Vec model. With
the feed-forward neural network models, we will limit the length of
the tokenized tweet as the input to the network has to be a fixed
sized. Tweets above this length threshold will be dropped from the
dataset and tweets below the threshold will be padded.

\subsection{Feed-Forward Neural Network} The first approach we will
take to classify tweets will be a feed-forward neural network
architecture. We will try two different implementations with a single
hidden layer and multiple hidden layers.

\subsubsection{Single Hidden Layer} The single layered feed-forward
neural network will be fully connected and have as input 20,000 sized
vector with a single hidden layer of 200 neurons, and an output
layer with one neuron for prediction. The activation function for the
hidden layer will be the Rectified Linear Unit (ReLU) function and the
output layer will have the activation function as sigmoid to make the
prediction.

\subsubsection{Multiple Hidden Layers} The multiple hidden layered
feed-forward neural network will be fully connected and have as input
20,000 sized vector with three hidden layers of 200 neurons each, and
an output layer with one neuron for prediction. Similarly as the
single hidden layer feed-forward neural network, the activaction
function for the hidden layers will be ReLU and the output layer will
be sigmoid.

\subsection{Long Short-Term Memory}

\subsection{Bidirectional Encoder Representations from Transformers (BERT)}
BERT is available for download on Github. The input file for this tool should be in a \textit{.tsv} file format. BERT accepts input (\textit{train.tsv} and \textit{dev.tsv}) in a certain format.  The input format that BERT accepts is given below:
\begin{itemize}
\item Column 1 is the id for a row.
\item Column 2 is a label for the row. This is an integer. These are classification labels that our classifier aims to predict.
\item Column 3 is a column of all the same letter {\textemdash}  this is a throw-away column that we need to include because the BERT model expects it.
\item Column 4 the text examples that we want to classify.
\end{itemize}
The file \textit{test.tsv} should be in another format. The format is given below:
\begin{itemize}
\item Column 1 is the id for a row.
\item Column 2 is the text that we want to classify.
\end{itemize}
There are four BERT ``base'' models which have different pre-trained weights. We picked ``cased'' and ``uncased''. These are use for different letter cases.
A script file is created to run BERT.  This script file contains the location of the input data files (\textit{.tsv}). It also contains entry to specify the location of BERT pre-trained weights.

On running this script file, we got an error. The error  is ``Assign requires shapes of both tensors to match.''. On searching for solution for this error, we found that this error may be caused because TensorFlow does not delete previous checkpoints. However, we have not found the correct solution to this problem.