\section{Methods}
\label{sec:methods}

\subsection{Dataset} The dataset we will be using for our training and
evaluation will be the My Information Bubble (MIB) dataset
\citep{Cresci:2017:PSS:3041021.3055135}. It is a collection of genuine
and spam tweets from bot accounts. There are thousands of accounts
with millions of tweets. In one approach, we will split the dataset by accounts with a
$90$/$10$ split for training and validation data. In another approach, we will split the dataset into three sets (training, validation and testing). The split will be done by accounts level with a $80$/$20$ for training and testing data, then a split for the training data by $80$/$20$ for training and validation data. 

\subsection{Pre-Processing} We will pre-process the tweets in order to
normalize their format and reduce vocabulary size when we create an
embedding.

\subsubsection{Parsing} To parse the tweets, we will use three different ways depending on the embedding that will be mentioned in the next section. 

In the first approach, we split our tweet sentence on the basis of comma and strip the data of double quotes. We used a Word2Vec embedding \citep{Mikolov:2013:DRW:2999792.2999959} which decreases the dimensionality to $100$ to represent one word. We tried two other approaches, since this approach did not result in good results in terms of ``Accuracy''.

In one approach, we employ the
text-processing tool Ekphrasis
\citep{baziotis-pelekis-doulkeridis:2017:SemEval2} that will perform
tokenization, normalization and word segmentation. Using Ekphrasis
allows for a uniform format for the tweets and abstracts away
usernames and emojis which will decrease the vocabulary size that will
be used in the embedding. 

In another approach, we will preprocess the tweets based on the preprocessing script that is done by the creators of Global Vectors for Word Represention (GloVE) \citep{pennington-etal-2014-glove}. This will allow us to use the pre-trained word vectors for Twitter data provided by GloVE. 


\subsubsection{Embedding} Since our vocabulary size will be large as
the breadth of language, slang, and domain specific words is diverse.
Using a one-hot-encoding, would increase the dimensionality of the
input by a large margin. To overcome this obstacle, we used word embedding. We can either train the word embedding in our data or use a pre trained one. In this project we tried both of these approaches. In two approaches, we used a Word2Vec embedding \citep{Mikolov:2013:DRW:2999792.2999959} which decreases the dimensionality to $100$ to represent one word. In particular, we train a Word2Vec
model\footnote{https://radimrehurek.com/gensim/models/word2vec.html} on our tweets and use the embedding to encode them for input to the model. 

In another approach, we used the pre-trained GloVE specific in twitter data which encode the vocabularies in $100$ dimension vectors \footnote{https://nlp.stanford.edu/projects/glove/}. In particular, we used a set of the embedding from the pre-trained GloVE to encode our tweets to fed them as input to our model. 

\subsection{Pruning} We pruned tweets from our dataset that fall
outside the vocabularies that was learning from our Word2Vec model. With
the feed-forward neural network models, we limit the length of
the tokenized tweet as the input to the network has to be a fixed
sized. Tweets above this length threshold have been dropped from the
dataset and tweets below the threshold will be padded. While with the pre-trained Glove approach, we used zero vector to encode any vocabulary that falls outside the pre-trained Glove. 

\subsection{Feed-Forward Neural Network} For the first two approaches we have classified tweets in a feed-forward neural network
architecture. We have tried two different implementations with a single
hidden layer and multiple hidden layers.

\subsubsection{Single Hidden Layer} The single layered feed-forward
neural network are fully connected and have as input $20000$ sized
vector with a single hidden layer of $200$ neurons, and an output
layer with one neuron for prediction. The activation function for the
hidden layer will be the Rectified Linear Unit (ReLU) function and the
output layer will have the activation function as sigmoid to make the
prediction.

\subsubsection{Multiple Hidden Layers} The multiple hidden layered
feed-forward neural network are fully connected and have as input
$20000$ sized vector with three hidden layers of $200$ neurons each, and
an output layer with one neuron for prediction. Similarly, as the
single hidden layer feed-forward neural network, the activation
function for the hidden layers are ReLU and the output layer is sigmoid.

\subsection{Long Short-Term Memory (LSTM)}
In this model, we used pre-trained GloVE to encode the tweets text. So, the first part was preprocessing the tweets text to match the preprocessing for GloVE tweets, as mentioned in the parsing section. The labels were added to the dataset to recognize the ones from the genuine tweetsâ€™ dataset and the spam ones. The split of the data was done following the second approach mentioned in the dataset section. We developed the model using Keras framework  \citep{chollet2015keras}:
First layer was an embedding layer. This layer will take as an input : the maximum length of a tweet , which has been set in this approach to $140$, the number of vocabularies that we have, in this approach the counting was done for the training set, and the dimension of the dense embedding, in this approach is set to $100$. Since we used a pre trained Glove for the embedding, we provided this layer with the weights for the embedding and set them to be untrainable. 
The second layer is an LSTM layer with $100$ neurons and a $20$\% dropout. 
The final, output, layer has a single neuron for prediction with sigmoid function.


\subsection{Bidirectional Encoder Representations from Transformers (BERT)}
BERT is available for download on Github. The input file for this tool should be in a \textit{.tsv} file format. BERT accepts input (\textit{train.tsv} and \textit{dev.tsv}) in a certain format.  The input format that BERT accepts is given below:
\begin{itemize}
\item Column 1 is the id for a row.
\item Column 2 is a label for the row. This is an integer. These are classification labels that our classifier aims to predict.
\item Column 3 is a column of all the same letter {\textemdash}  this is a throw-away column that we need to include because the BERT model expects it.
\item Column 4 the text examples that we want to classify.
\end{itemize}
The file \textit{test.tsv} should be in another format. The format is given below:
\begin{itemize}
\item Column 1 is the id for a row.
\item Column 2 is the text that we want to classify.
\end{itemize}
There are four BERT ``base'' models which have different pre-trained weights. We picked ``cased'' and ``uncased''. These are use for different letter cases.
A script file is created to run BERT.  This script file contains the location of the input data files (\textit{.tsv}). It also contains entry to specify the location of BERT pre-trained weights.

On running this script file, we got an error. The error  is ``Assign requires shapes of both tensors to match.''. On searching for solution for this error, we found that this error may be caused because TensorFlow does not delete previous checkpoints. However, we have not found the correct solution to this problem.