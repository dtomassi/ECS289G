\section{Methods}
\label{sec:methods}

\subsection{Dataset} The dataset we will be using for our training and
evaluation will be the My Information Bubble (MIB) dataset
\citep{Cresci:2017:PSS:3041021.3055135}. It is a collection of genuine
and spam tweets from bot accounts. There are thousands of accounts
with millions of tweets. We will split the dataset by accounts with a
90/10 split for training and validation data.

\subsection{Pre-Processing} We will pre-process the tweets in order to
normalize their format and reduce vocabulary size when we create an
embedding.

\subsubsection{Parsing} To parse the tweets, we employ the
text-processing tool Ekphrasis
\citep{baziotis-pelekis-doulkeridis:2017:SemEval2} that will performn
tokenization, normalization and word segmentation. Using Ekphrasis
allows for a uniform format for the tweets and abstracts away
usernames and emojis which will decress the vocabulary size that will
be used in the embedding.

\subsubsection{Embedding} Since our vocabulary size will be large as
the breadth of language, slang, and domain specific words is diverse.
Using a one-hot-encoding, would increase the dimensonality of the
input by a large margin. To overcome this obstricle, we will be use a
Word2Vec embedding \citep{Mikolov:2013:DRW:2999792.2999959} which will
decrease the dimensonality to 100 to represent one word. In particular
we will train a Word2Vec
model\footnote{https://radimrehurek.com/gensim/models/word2vec.html}
on our tweets and use the embedding to encode them for input to the
model.

\subsection{Pruning} We will prune tweets from our dataset that fall
outside the vocabular that was learning from our Word2Vec model. With
the feed-forward neural network models, we will limit the length of
the tokenized tweet as the input to the network has to be a fixed
sized. Tweets above this length threshold will be dropped from the
dataset and tweets below the threshold will be padded.

\subsection{Feed-Forward Neural Network} The first approach we will
take to classify tweets will be a feed-forward neural network
architecture. We will try two different implementations with a single
hidden layer and multiple hidden layers.

\subsubsection{Single Hidden Layer} The single layered feed-forward
neural network will be fully connected and have as input 20,000 sized
vector with a single hidden layer of 200 neurons, and an output
layer with one neuron for prediction. The activation function for the
hidden layer will be the Rectified Linear Unit (ReLU) function and the
output layer will have the activation function as sigmoid to make the
prediction.

\subsubsection{Multiple Hidden Layers} The multiple hidden layered
feed-forward neural network will be fully connected and have as input
20,000 sized vector with three hidden layers of 200 neurons each, and
an output layer with one neuron for prediction. Similarly as the
single hidden layer feed-forward neural network, the activaction
function for the hidden layers will be ReLU and the output layer will
be sigmoid.

\subsection{Long Short-Term Memory}

\subsection{Bidirectional Encoder Representations from Transformers}
