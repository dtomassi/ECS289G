{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports:\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import regex as re\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "import tensorflow as tf\n",
    "\n",
    "#packages for matrices:\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------Loading the data: ---------------#\n",
    "\n",
    "# 1.open the files and creat the dataframe:\n",
    "\n",
    "# open the files by panda library :   https://towardsdatascience.com/natural-language-processing-classification-using-deep-learning-and-word2vec-50cbadd3bd6a\n",
    "hamFiles = pd.read_csv(\"Datasets_files/g_tweets.csv\")\n",
    "spamFiles1 = pd.read_csv(\"Datasets_files/sp_soci_2_tweets.csv\")\n",
    "spamFiles2 = pd.read_csv(\"Datasets_files/sp_tra_1_tweets.csv\")\n",
    "\n",
    "#2.drop all the columns except the tweets text\n",
    "hamFiles=hamFiles[['text','user_id']]\n",
    "spamFiles1=spamFiles1[['text','user_id']]\n",
    "spamFiles2=spamFiles2[['text','user_id']]\n",
    "\n",
    "# 3.create the labels:\n",
    "ones = []   # for ham\n",
    "zeros_1 = []   # for spam1\n",
    "zeros_2 = []   # for spam2\n",
    "\n",
    "for i in range(hamFiles.shape[0]):\n",
    "    ones.append(1)\n",
    "for i in range(spamFiles1.shape[0]):    \n",
    "    zeros_1.append(0)\n",
    "for i in range(spamFiles2.shape[0]):    \n",
    "    zeros_2.append(0)\n",
    "\n",
    "hamFiles['label'] = ones\n",
    "spamFiles1['label'] = zeros_1\n",
    "spamFiles2['label'] = zeros_2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428542\n",
      "145094\n",
      "2839362\n",
      "573636\n"
     ]
    }
   ],
   "source": [
    "# 4. split ham files to take only number of tweets = spam tweets avialble:\n",
    "print(spamFiles1.shape[0])\n",
    "print(spamFiles2.shape[0])\n",
    "print(hamFiles.shape[0])\n",
    "numOfSamples=spamFiles1.shape[0]+spamFiles2.shape[0]\n",
    "hamFiles, hamFiles_unused = hamFiles[:numOfSamples], hamFiles[numOfSamples:]\n",
    "print(hamFiles.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @morningJewshow: Speaking about Jews and co...</td>\n",
       "      <td>678033.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This age/face recognition thing..no reason pla...</td>\n",
       "      <td>678033.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Only upside of the moment I can think of is th...</td>\n",
       "      <td>678033.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you're going to think about+create experien...</td>\n",
       "      <td>678033.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Watching a thread on FB about possible future ...</td>\n",
       "      <td>678033.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   user_id  label\n",
       "0  RT @morningJewshow: Speaking about Jews and co...  678033.0      1\n",
       "1  This age/face recognition thing..no reason pla...  678033.0      1\n",
       "2  Only upside of the moment I can think of is th...  678033.0      1\n",
       "3  If you're going to think about+create experien...  678033.0      1\n",
       "4  Watching a thread on FB about possible future ...  678033.0      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. merge all the files in one dataset:\n",
    "tweets = hamFiles\n",
    "tweets = tweets.append(spamFiles1)\n",
    "tweets = tweets.append(spamFiles2)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- preprocessing the data -------------------------#\n",
    "\n",
    "# 1. cleaning the tweets the same way the preprocessing done for the pre trained Glove tweets : https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "# Functions for the cleaning:    https://gist.github.com/tokestermw/cb87a97113da12acb388\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \" {} \".format(hashtag_body.lower())\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps>\"\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"#\\S+\", hashtag)\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "\n",
    "    ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
    "    # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
    "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "\n",
    "    return text.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     rt <allcaps> <user>: speaking about jews and c...\n",
      "1     this age / face recognition thing. <repeat>no ...\n",
      "2     only upside of the moment i can think of is th...\n",
      "3     if you're going to think about+create experien...\n",
      "4     watching a thread on fb <allcaps> about possib...\n",
      "5                                      don't. ok? <url>\n",
      "6     rt <allcaps> <user>: <hashtag> enoughsenough \"...\n",
      "7     rt <allcaps> <user>: kriss kross once rapped \"...\n",
      "8     rt <allcaps> <user>: watch baltimore native <u...\n",
      "9     <user> <user> i didn't realize anyone has ever...\n",
      "10    rt <allcaps> <user>: here's my one paragraph r...\n",
      "11    <user> i stand by you. i don't want men tellin...\n",
      "12                              <user> thank you becca.\n",
      "13    <user> i dont' ask for rt <allcaps>s gen, but ...\n",
      "14    not only$ but acts+messages from people that s...\n",
      "15    to live with untreated ptsd <allcaps> is to fe...\n",
      "16    <user> is also a columbine survivor. miles awa...\n",
      "17    <user> is old skool web. a good web citizen. a...\n",
      "18    one reason my life has gotten better is the su...\n",
      "19    rt <allcaps> <user>: jon stewart got it right ...\n",
      "20                                    <user> thank you.\n",
      "21    rt <allcaps> <user>: hundreds of people are in...\n",
      "22    \"sexy. but crazy.\" i play with makeup for the ...\n",
      "23    \"sexy. but crazy.\" i play with makeup for the ...\n",
      "24    \"sexy. but crazy.\"  i play with makeup for the...\n",
      "25    <user> when will oths <allcaps> stop with the ...\n",
      "26    infuriating to have the oakland pd <allcaps> t...\n",
      "27    the best part of heading to toronto for a mont...\n",
      "28    oakland tech is once again got the stadium lev...\n",
      "29      the seinfeld series is going to be on hulu<url>\n",
      "                            ...                        \n",
      "70    rt <allcaps> <user>: <user> similarly, though ...\n",
      "71    the web i love mt <allcaps> <user>  <elong>so ...\n",
      "72    <user> this reminds me of how people responded...\n",
      "73                                 <user> congrats jon!\n",
      "74    one of my fave pages on the net: <user> short ...\n",
      "75    <user> <user> i disagree that tech makes peopl...\n",
      "76    rt <allcaps> <user>: always a fagen fan but hi...\n",
      "77    twitter doesn't have a good enough history re ...\n",
      "78    if you don't believe in hell, it's hard to fin...\n",
      "79    really twitter? who wrote this encapsulation o...\n",
      "80    <user> interesting the way the world of capita...\n",
      "81    great piece from <user> on taking spirituality...\n",
      "82      <user> the maintenance change is very appealing\n",
      "83    <user> i'm guessing hundreds to <number> but <...\n",
      "84    rt <allcaps> <user>: american airlines' qualit...\n",
      "85    rt <allcaps> <user>: sunday in <hashtag>  berl...\n",
      "86    picked up another spectacular middle-aged acco...\n",
      "87    <user> <user> privileging of corporate liabili...\n",
      "88    <user> <user> are we to do things together or ...\n",
      "89    <user> <user> i believe it's bigger and broade...\n",
      "90    <user> <user> . <repeat>sustained value. disma...\n",
      "91    <user> <user> i think a lot i the problem is i...\n",
      "92    <user> <user> <user> it's just what makes life...\n",
      "93    <user> <user> <user> <user> cyberselfish is a ...\n",
      "94    <user> i saw women do the same thing re: holly...\n",
      "95    oy vey. the price. â€œ<user>: <user> hey, i sele...\n",
      "96                          <user> thanks that's v kind\n",
      "97    <user> the web felt clearly like it was about ...\n",
      "98    <user> that's really interesting.early web tim...\n",
      "99    rt <allcaps> <user>: <user> ah. see i was in m...\n",
      "Name: clean_tweets, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# start cleaning the tweets: \n",
    "tweets['clean_tweets'] = tweets['text'].astype(str).apply(tokenize)        \n",
    "print(tweets['clean_tweets'].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.spliting data in level of user ID:    https://stackoverflow.com/questions/54797508/how-to-generate-a-train-test-split-based-on-a-group-id\n",
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7).split(tweets, groups=tweets['user_id']))\n",
    "\n",
    "train = tweets.iloc[train_inds]\n",
    "test = tweets.iloc[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(901954,)\n",
      "(901954,)\n",
      "0         rt <allcaps> <user>: speaking about jews and c...\n",
      "1         this age / face recognition thing. <repeat>no ...\n",
      "2         only upside of the moment i can think of is th...\n",
      "3         if you're going to think about+create experien...\n",
      "4         watching a thread on fb <allcaps> about possib...\n",
      "5                                          don't. ok? <url>\n",
      "6         rt <allcaps> <user>: <hashtag> enoughsenough \"...\n",
      "7         rt <allcaps> <user>: kriss kross once rapped \"...\n",
      "8         rt <allcaps> <user>: watch baltimore native <u...\n",
      "9         <user> <user> i didn't realize anyone has ever...\n",
      "10        rt <allcaps> <user>: here's my one paragraph r...\n",
      "11        <user> i stand by you. i don't want men tellin...\n",
      "12                                  <user> thank you becca.\n",
      "13        <user> i dont' ask for rt <allcaps>s gen, but ...\n",
      "14        not only$ but acts+messages from people that s...\n",
      "15        to live with untreated ptsd <allcaps> is to fe...\n",
      "16        <user> is also a columbine survivor. miles awa...\n",
      "17        <user> is old skool web. a good web citizen. a...\n",
      "18        one reason my life has gotten better is the su...\n",
      "19        rt <allcaps> <user>: jon stewart got it right ...\n",
      "20                                        <user> thank you.\n",
      "21        rt <allcaps> <user>: hundreds of people are in...\n",
      "22        \"sexy. but crazy.\" i play with makeup for the ...\n",
      "23        \"sexy. but crazy.\" i play with makeup for the ...\n",
      "24        \"sexy. but crazy.\"  i play with makeup for the...\n",
      "25        <user> when will oths <allcaps> stop with the ...\n",
      "26        infuriating to have the oakland pd <allcaps> t...\n",
      "27        the best part of heading to toronto for a mont...\n",
      "28        oakland tech is once again got the stadium lev...\n",
      "29          the seinfeld series is going to be on hulu<url>\n",
      "                                ...                        \n",
      "145043    families usa <allcaps> lauds new improvements ...\n",
      "145044    lawsuit shows insurer targeted hiv <allcaps> p...\n",
      "145045    also in global health news: drought in s. chin...\n",
      "145046    robotic surgery in male infertility and chroni...\n",
      "145047    cmace <allcaps> publishes information on obesi...\n",
      "145048    today's opinions and editorials: stop the blee...\n",
      "145049    cancer-themed issue of jama <allcaps> features...\n",
      "145050    russia says iran letting chances for dialogue ...\n",
      "145051    extreme obesity affecting more children at you...\n",
      "145052    'flying vaccinator': can genetically engineere...\n",
      "145053    reform outlook improves as groups, lawmakers p...\n",
      "145054    global partners join forces to speed developme...\n",
      "145055    impact of ethnicity on primary treatment choic...\n",
      "145056    militants declare campaign vs iraqi political ...\n",
      "145057    fda <allcaps> issues final rule restricting ac...\n",
      "145058    risk of child accidents in the home: targeted ...\n",
      "145059    breakthrough for babies born with severe cleft...\n",
      "145060    study shows prevenar <number> is immunogenic i...\n",
      "145061    alkermes announces initiation of multidose pha...\n",
      "145062    acucela and otsuka pharmaceutical receive fda ...\n",
      "145063    greatbatch medical receives <number>(k) cleara...\n",
      "145064    quintiles examines asia-pacific drug developme...\n",
      "145065    human genome sciences announces results of ran...\n",
      "145066    good morning america forever <number> haul par...\n",
      "145067    what <allcaps> would <allcaps> you <allcaps> d...\n",
      "145068    best <allcaps> lip <allcaps> gloss <allcaps> e...\n",
      "145069    israeli settlement crisis clouds mideast talks...\n",
      "145070    bomb damages right-wing group's athens office ...\n",
      "145071    fox <allcaps> insults obama- president gets in...\n",
      "145072    aged care nurses suffer almost <number> times ...\n",
      "Name: clean_tweets, Length: 245318, dtype: object\n",
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "5         1\n",
      "6         1\n",
      "7         1\n",
      "8         1\n",
      "9         1\n",
      "10        1\n",
      "11        1\n",
      "12        1\n",
      "13        1\n",
      "14        1\n",
      "15        1\n",
      "16        1\n",
      "17        1\n",
      "18        1\n",
      "19        1\n",
      "20        1\n",
      "21        1\n",
      "22        1\n",
      "23        1\n",
      "24        1\n",
      "25        1\n",
      "26        1\n",
      "27        1\n",
      "28        1\n",
      "29        1\n",
      "         ..\n",
      "145043    0\n",
      "145044    0\n",
      "145045    0\n",
      "145046    0\n",
      "145047    0\n",
      "145048    0\n",
      "145049    0\n",
      "145050    0\n",
      "145051    0\n",
      "145052    0\n",
      "145053    0\n",
      "145054    0\n",
      "145055    0\n",
      "145056    0\n",
      "145057    0\n",
      "145058    0\n",
      "145059    0\n",
      "145060    0\n",
      "145061    0\n",
      "145062    0\n",
      "145063    0\n",
      "145064    0\n",
      "145065    0\n",
      "145066    0\n",
      "145067    0\n",
      "145068    0\n",
      "145069    0\n",
      "145070    0\n",
      "145071    0\n",
      "145072    0\n",
      "Name: label, Length: 245318, dtype: int64\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "6    1\n",
      "7    1\n",
      "8    1\n",
      "9    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train=train['clean_tweets']\n",
    "X_test=test['clean_tweets']\n",
    "y_train=train['label']\n",
    "y_test=test['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.representing tweets as sequences prepairing for Glove embading:\n",
    "\n",
    "# did not specify max num of words in vocab dic so it will include all\n",
    "tk= Tokenizer(filters='!\"\\t\\n')\n",
    "\n",
    "tk.fit_on_texts(X_train) # to build our data vocab\n",
    "X_train_seq = tk.texts_to_sequences(X_train) # to convert the text into seq of numbers related to the vocab index\n",
    "X_test_seq = tk.texts_to_sequences(X_test)\n",
    "\n",
    "#max length of tweet 140 , the updated tweitter is more tho\n",
    "X_train_seq_trunc = pad_sequences(X_train_seq,maxlen=140) #pad the short tweets so all the seq of same length\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq,maxlen=140)\n",
    "\n",
    "\n",
    "# split the training set into training and validation \n",
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train, test_size=0.2, random_state=37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Embading : \n",
    "\n",
    "# 1. load the pretraind Glove to a dictionary : https://towardsdatascience.com/word-embeddings-for-sentiment-analysis-65f42ea5d26e\n",
    "emb_dict = {}\n",
    "glove = open(\"Glove_pretrined/glove.twitter.27B.100d.txt\")\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.generate the embading matrix for our data from the Glove dictonary:\n",
    "NB_WORDS = len(tk.word_index)  # Parameter indicating the number of words we'll put in the dictionary, usully I see less but I try now all\n",
    "GLOVE_DIM= 100;\n",
    "emb_matrix = np.zeros((NB_WORDS+1, GLOVE_DIM))   # set the defult embading for the words to be zeros\n",
    "for w, i in tk.word_index.items():\n",
    "    if i < NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect    # set the embading for the word to the one from the Glove\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 140, 100)          42613700  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 140, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 136, 64)           32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 34, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,711,865\n",
      "Trainable params: 98,165\n",
      "Non-trainable params: 42,613,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ------ Creating the model          : https://medium.com/@sabber/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa\n",
    "MAX_LEN=140   #the prevuilsly decided length of the max seq of text\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(NB_WORDS+1,100, input_length=MAX_LEN, weights=[emb_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(100))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 721563 samples, validate on 180391 samples\n",
      "Epoch 1/3\n",
      "721563/721563 [==============================] - 1709s 2ms/step - loss: 0.3121 - accuracy: 0.8607 - val_loss: 0.2716 - val_accuracy: 0.8830\n",
      "Epoch 2/3\n",
      "721563/721563 [==============================] - 2095s 3ms/step - loss: 0.2745 - accuracy: 0.8816 - val_loss: 0.2625 - val_accuracy: 0.8886\n",
      "Epoch 3/3\n",
      "721563/721563 [==============================] - 1666s 2ms/step - loss: 0.2656 - accuracy: 0.8860 - val_loss: 0.2577 - val_accuracy: 0.8906\n",
      "Validation accuracy: 0.8906320333480835, loss: 0.2577085436462187\n"
     ]
    }
   ],
   "source": [
    "# Create callback for early stopping on validation loss. If the loss does not decrease in two consecutive tries, stop training.\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
    "\n",
    "# Fit train data into the model:\n",
    "###^^^^^^^^^^^^ I see examples with larger epoches$$$$come back to check how to decide this \n",
    "history=model_glove.fit(X_train_emb, y_train_emb, validation_data=(X_valid_emb, y_valid_emb),callbacks=callbacks, epochs = 3)\n",
    "# Print results.\n",
    "history = history.history\n",
    "print('Validation accuracy: {acc}, loss: {loss}'.format(acc=history['val_accuracy'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "# Save model.\n",
    "model_glove.save('Spambot_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245318/245318 [==============================] - 116s 471us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4224773896021752, 0.8285979628562927]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test by the test data without rebuilding the word embading dic:\n",
    "model_glove.evaluate(x=X_test_seq_trunc, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.828598\n",
      "Precision: 0.805593\n",
      "Recall: 0.864927\n",
      "F1 score: 0.834206\n"
     ]
    }
   ],
   "source": [
    "#-----Calculating the Recall,perciosion, F score:\n",
    "# predict probabilities for test set    https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/\n",
    "yhat_probs = model_glove.predict(X_test_seq_trunc, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model_glove.predict_classes(X_test_seq_trunc, verbose=0)\n",
    "\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
