{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports:\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import regex as re\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "import tensorflow as tf\n",
    "\n",
    "#packages for matrices:\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "//anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @morningJewshow: Speaking about Jews and co...</td>\n",
       "      <td>678033.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This age/face recognition thing..no reason pla...</td>\n",
       "      <td>678033.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Only upside of the moment I can think of is th...</td>\n",
       "      <td>678033.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you're going to think about+create experien...</td>\n",
       "      <td>678033.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Watching a thread on FB about possible future ...</td>\n",
       "      <td>678033.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   user_id  label\n",
       "0  RT @morningJewshow: Speaking about Jews and co...  678033.0      1\n",
       "1  This age/face recognition thing..no reason pla...  678033.0      1\n",
       "2  Only upside of the moment I can think of is th...  678033.0      1\n",
       "3  If you're going to think about+create experien...  678033.0      1\n",
       "4  Watching a thread on FB about possible future ...  678033.0      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------Loading the data: ---------------#\n",
    "\n",
    "# 1.open the files and creat the dataframe:\n",
    "\n",
    "# open the files by panda library : \n",
    "hamFiles = pd.read_csv(\"Datasets_files/g_tweets.csv\")\n",
    "spamFiles1 = pd.read_csv(\"Datasets_files/sp_soci_2_tweets.csv\")\n",
    "spamFiles2 = pd.read_csv(\"Datasets_files/sp_tra_1_tweets.csv\")\n",
    "\n",
    "#2.drop all the columns except the tweets text\n",
    "hamFiles=hamFiles[['text','user_id']]\n",
    "spamFiles1=spamFiles1[['text','user_id']]\n",
    "spamFiles2=spamFiles2[['text','user_id']]\n",
    "\n",
    "# 3.create the labels:\n",
    "ones = []   # for ham\n",
    "zeros_1 = []   # for spam1\n",
    "zeros_2 = []   # for spam2\n",
    "\n",
    "for i in range(hamFiles.shape[0]):\n",
    "    ones.append(1)\n",
    "for i in range(spamFiles1.shape[0]):    \n",
    "    zeros_1.append(0)\n",
    "for i in range(spamFiles2.shape[0]):    \n",
    "    zeros_2.append(0)\n",
    "\n",
    "hamFiles['label'] = ones\n",
    "spamFiles1['label'] = zeros_1\n",
    "spamFiles2['label'] = zeros_2\n",
    "\n",
    "# 4. split ham files to take only number of tweets = spam tweets avialble:\n",
    "\n",
    "numOfSamples=spamFiles1.shape[0]+spamFiles2.shape[0]\n",
    "hamFiles, hamFiles_unused = hamFiles[:numOfSamples], hamFiles[numOfSamples:]\n",
    "\n",
    "# 5. merge all the files in one dataset:\n",
    "tweets = hamFiles\n",
    "tweets = tweets.append(spamFiles1)\n",
    "tweets = tweets.append(spamFiles2)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     rt <allcaps> <user>: speaking about jews and c...\n",
      "1     this age / face recognition thing. <repeat>no ...\n",
      "2     only upside of the moment i can think of is th...\n",
      "3     if you're going to think about+create experien...\n",
      "4     watching a thread on fb <allcaps> about possib...\n",
      "5                                      don't. ok? <url>\n",
      "6     rt <allcaps> <user>: <hashtag> enoughsenough \"...\n",
      "7     rt <allcaps> <user>: kriss kross once rapped \"...\n",
      "8     rt <allcaps> <user>: watch baltimore native <u...\n",
      "9     <user> <user> i didn't realize anyone has ever...\n",
      "10    rt <allcaps> <user>: here's my one paragraph r...\n",
      "11    <user> i stand by you. i don't want men tellin...\n",
      "12                              <user> thank you becca.\n",
      "13    <user> i dont' ask for rt <allcaps>s gen, but ...\n",
      "14    not only$ but acts+messages from people that s...\n",
      "15    to live with untreated ptsd <allcaps> is to fe...\n",
      "16    <user> is also a columbine survivor. miles awa...\n",
      "17    <user> is old skool web. a good web citizen. a...\n",
      "18    one reason my life has gotten better is the su...\n",
      "19    rt <allcaps> <user>: jon stewart got it right ...\n",
      "20                                    <user> thank you.\n",
      "21    rt <allcaps> <user>: hundreds of people are in...\n",
      "22    \"sexy. but crazy.\" i play with makeup for the ...\n",
      "23    \"sexy. but crazy.\" i play with makeup for the ...\n",
      "24    \"sexy. but crazy.\"  i play with makeup for the...\n",
      "25    <user> when will oths <allcaps> stop with the ...\n",
      "26    infuriating to have the oakland pd <allcaps> t...\n",
      "27    the best part of heading to toronto for a mont...\n",
      "28    oakland tech is once again got the stadium lev...\n",
      "29      the seinfeld series is going to be on hulu<url>\n",
      "                            ...                        \n",
      "70    rt <allcaps> <user>: <user> similarly, though ...\n",
      "71    the web i love mt <allcaps> <user>  <elong>so ...\n",
      "72    <user> this reminds me of how people responded...\n",
      "73                                 <user> congrats jon!\n",
      "74    one of my fave pages on the net: <user> short ...\n",
      "75    <user> <user> i disagree that tech makes peopl...\n",
      "76    rt <allcaps> <user>: always a fagen fan but hi...\n",
      "77    twitter doesn't have a good enough history re ...\n",
      "78    if you don't believe in hell, it's hard to fin...\n",
      "79    really twitter? who wrote this encapsulation o...\n",
      "80    <user> interesting the way the world of capita...\n",
      "81    great piece from <user> on taking spirituality...\n",
      "82      <user> the maintenance change is very appealing\n",
      "83    <user> i'm guessing hundreds to <number> but <...\n",
      "84    rt <allcaps> <user>: american airlines' qualit...\n",
      "85    rt <allcaps> <user>: sunday in <hashtag>  berl...\n",
      "86    picked up another spectacular middle-aged acco...\n",
      "87    <user> <user> privileging of corporate liabili...\n",
      "88    <user> <user> are we to do things together or ...\n",
      "89    <user> <user> i believe it's bigger and broade...\n",
      "90    <user> <user> . <repeat>sustained value. disma...\n",
      "91    <user> <user> i think a lot i the problem is i...\n",
      "92    <user> <user> <user> it's just what makes life...\n",
      "93    <user> <user> <user> <user> cyberselfish is a ...\n",
      "94    <user> i saw women do the same thing re: holly...\n",
      "95    oy vey. the price. â€œ<user>: <user> hey, i sele...\n",
      "96                          <user> thanks that's v kind\n",
      "97    <user> the web felt clearly like it was about ...\n",
      "98    <user> that's really interesting.early web tim...\n",
      "99    rt <allcaps> <user>: <user> ah. see i was in m...\n",
      "Name: clean_tweets, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# -------------- preprocessing the data -------------------------#\n",
    "\n",
    "# 1. cleaning the tweets the same way the preprocessing done for the pre trained Glove tweets : https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "# Functions for the cleaning:    FROM: https://gist.github.com/tokestermw/cb87a97113da12acb388\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \" {} \".format(hashtag_body.lower())\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps>\"\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"#\\S+\", hashtag)\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "\n",
    "    ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
    "    # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
    "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "\n",
    "    return text.lower()\n",
    "#------ end The function from : https://gist.github.com/tokestermw/cb87a97113da12acb388\n",
    "\n",
    "# start cleaning the tweets: \n",
    "tweets['clean_tweets'] = tweets['text'].astype(str).apply(tokenize)        \n",
    "print(tweets['clean_tweets'].head(100))\n",
    "\n",
    "# 2.spliting data in level of user ID:    https://stackoverflow.com/questions/54797508/how-to-generate-a-train-test-split-based-on-a-group-id\n",
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7).split(tweets, groups=tweets['user_id']))\n",
    "\n",
    "train = tweets.iloc[train_inds]\n",
    "test = tweets.iloc[test_inds]\n",
    "\n",
    "# 4.specify the columns\n",
    "X_train=train['clean_tweets']\n",
    "X_test=test['clean_tweets']\n",
    "y_train=train['label']\n",
    "y_test=test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.representing tweets as sequences prepairing for Glove embading:\n",
    "\n",
    "# did not specify max num of words in vocab dic so it will include all\n",
    "tk= Tokenizer(filters='!\"\\t\\n')\n",
    "\n",
    "tk.fit_on_texts(X_train) # to build our data vocab\n",
    "X_train_seq = tk.texts_to_sequences(X_train) # to convert the text into seq of numbers related to the vocab index\n",
    "X_test_seq = tk.texts_to_sequences(X_test)\n",
    "\n",
    "#max length of tweet 140 , the updated tweitter is more tho\n",
    "X_train_seq_trunc = pad_sequences(X_train_seq,maxlen=140) #pad the short tweets so all the seq of same length\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq,maxlen=140)\n",
    "\n",
    "\n",
    "# split the training set into training and validation \n",
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train, test_size=0.2, random_state=37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Embading : \n",
    "\n",
    "# 1. load the pretraind Glove to a dictionary :    learned from https://towardsdatascience.com/word-embeddings-for-sentiment-analysis-65f42ea5d26e\n",
    "emb_dict = {}\n",
    "glove = open(\"Glove_pretrined/glove.twitter.27B.100d.txt\")\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()\n",
    "\n",
    "# 2.generate the embading matrix for our data from the Glove dictonary:\n",
    "NB_WORDS = len(tk.word_index)  # Parameter indicating the number of words we'll put in the dictionary, usully I see less but I try now all\n",
    "GLOVE_DIM= 100;\n",
    "emb_matrix = np.zeros((NB_WORDS+1, GLOVE_DIM))   # set the defult embading for the words to be zeros\n",
    "for w, i in tk.word_index.items():\n",
    "    if i < NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect    # set the embading for the word to the one from the Glove\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 140, 100)          42613700  \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 42,694,201\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 42,613,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ------ Creating the model          \n",
    "MAX_LEN=140   #the prevuilsly decided length of the max seq of text\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(Embedding(NB_WORDS+1,100, input_length=MAX_LEN, weights=[emb_matrix], trainable=False))\n",
    "model_LSTM.add(LSTM(100,dropout=0.2))\n",
    "model_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "model_LSTM.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 721563 samples, validate on 180391 samples\n",
      "Epoch 1/5\n",
      "721563/721563 [==============================] - 3681s 5ms/step - loss: 0.3098 - accuracy: 0.8620 - val_loss: 0.2644 - val_accuracy: 0.8871\n",
      "Epoch 2/5\n",
      "721563/721563 [==============================] - 3253s 5ms/step - loss: 0.2648 - accuracy: 0.8871 - val_loss: 0.2523 - val_accuracy: 0.8932\n",
      "Epoch 3/5\n",
      " 31360/721563 [>.............................] - ETA: 45:54 - loss: 0.2560 - accuracy: 0.8906"
     ]
    }
   ],
   "source": [
    "# Create callback for early stopping on validation loss. If the loss does not decrease in two consecutive tries, stop training.\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
    "\n",
    "# Fit train data into the model:\n",
    "history=model_LSTM.fit(X_train_emb, y_train_emb,batch_size=40, validation_data=(X_valid_emb, y_valid_emb),callbacks=callbacks, epochs =5)\n",
    "# Print results.\n",
    "history = history.history\n",
    "print('Validation accuracy: {acc}, loss: {loss}'.format(acc=history['val_accuracy'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "# Save model.\n",
    "model_LSTM.save('Spambot_model_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245318/245318 [==============================] - 257s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43243726542944433, 0.8264660835266113]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test by the test data without rebuilding the word embading dic:Validation accuracy: 0.8979660868644714, loss: 0.24381882907302493\n",
    "model_LSTM.evaluate(x=X_test_seq_trunc, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.826466\n",
      "Precision: 0.800714\n",
      "Recall: 0.867944\n",
      "F1 score: 0.832974\n"
     ]
    }
   ],
   "source": [
    "#-----Calculating the Recall,perciosion, F score:\n",
    "# predict probabilities for test set    https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/\n",
    "yhat_probs = model_LSTM.predict(X_test_seq_trunc, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model_LSTM.predict_classes(X_test_seq_trunc, verbose=0)\n",
    "\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
